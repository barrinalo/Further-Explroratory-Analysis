{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Exploratory Analysis\n",
    "In the last handout we looked at how to obtain data, clean it, and explore some basic characteristics like mean and standard deviation as well as visualise a feature's distribution using a histogram. In this handout we will explore a couple more methods for exploring data and begin to introduce some categories of machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the iris data set and clean it again\n",
    "iris = pd.read_csv(\"iris.csv\",names=[\"Sepal_Length\",\"Sepal_Width\",\"Petal_Length\",\"Petal_Width\",\"Class\"])\n",
    "iris = iris.dropna()\n",
    "iris = iris[iris[\"Sepal_Width\"] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of exploring the data is to plot samples in a scatter plot like so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by classes\n",
    "\n",
    "# Plot a scatter with same x and y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the same feature against itself doesn't seem particularly useful, but what this does is plot all the points along a straight line, and this lets you see how well this particular feature separates the different classes. Normally, you wouldn't do this, because you are wasting one dimension, and you can get the same information by plotting 2 different features, but this just makes this idea a little clearer. Now in the space below, play around plotting different features against each other and see what results you can get. How many different feature pairs can you plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot using different features here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you played around with the scatter plots, you should have been able to see that samples of the same class tended to **cluster** together into blobs. Depending on which features you chose to plot, these blobs would be relatively separate or kind of overlapping. **Clustering** is another **task** in machine learning whose purpose is to find any natural groupings in the data without any labels. This is a form of **unsupervised learning** because you are working without labels. However, just plotting the samples using 2 features and saying \"oo there seems to be a couple of distinct blobs here\" is not clustering, this is just part of exploratory analysis where we get a feel of how well our clustering algorithms might work, and roughly how many natural clusters there might be in the data (which is a parameter that some clustering algorithms have that must be set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The curse of dimensionality\n",
    "One of the phrases that often gets bandied around is **the curse of dimensionality** which refers to the problem when you start to get too many features (hundreds to thousands) which makes statistical analysis/machine learning and exploration of the data hard and ineffective. How many feature pairs can you plot on a 2D plot, or even a 3D plot for that matter (don't think anyone can easily comprehend plots of higher dimension)? In the Iris data set, there are only 4 dimensions (features) so we are able to explore it quite easily. However, most data sets have much higher dimension. For instance, if you work with gene expression data, each human sample or cell you measure has around 20k+ genes that can have their expression measured, giving you a dimension of at least 20k+, and in some cases you can have higher dimensions, especially if you start to consider transcript expression instead (if each gene has multiple transcript variants). To explore the issue of dimensionality and further importance of data exploration in cleaning and preparing data, we will look at a new data set.\n",
    "## B and T cell microarray gene expression\n",
    "Take a look at this [paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3140206/), which the data set was taken from. The paper compares T and B cell gene expression using multiple microarray platforms. Try reading in the data and doing some exploratory analysis. (You may find using the **dataframe.T** property to be useful, it returns the transpose of the current dataframe. Additionally, using **dataframe.index.values** returns the row and **dataframe.columns.values** column names respectively.)\n",
    "\n",
    "1. Check the shape of the dataframe\n",
    "2. Find out how many samples and features\n",
    "3. Are there any names for the features\n",
    "4. Are there any missing or invalid values\n",
    "5. Do a cleanup by omitting any rows with invalid values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data and perform some exploratory analysis and cleanup\n",
    "bt_microarray = pd.read_csv(\"Painter_2012_B_and_T_Microarray.csv\").T # Transposes the data because we want genes as the features\n",
    "bt_microarray.columns = bt_microarray.loc[\"GeneSymbol\"] # Putting gene names in as column values\n",
    "bt_microarray = bt_microarray.iloc[7:,:] # Just helping you get rid of some rows we are not interested in at the moment\n",
    "for i in range(1, bt_microarray.shape[1]):\n",
    "    bt_microarray.iloc[:,i] = pd.to_numeric(bt_microarray.iloc[:,i]) # Just make sure each gene expression column is numeric data type\n",
    "# Print out the shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an idea of what each feature looks like\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an idea of what kind of samples we have and how many\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any invalid values?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup the data by dropping the genes with na values and find out the new shape of the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have done some data cleanup and preparation, we would like to explore the data a little. Do some scatter plots of random genes and see how well they separate the data. I have provided a list of colors below for you to color your scatter plots which should correspond to B and T cells as well as the platforms on which the data was collected. For each pair of genes you scatter, use each of the color sets by using the following command **plt.scatter(x,y,c=color_list)**. What can you tell about the data as it is?\n",
    "\n",
    "1. Which coloring scheme tends to result in more coherent clusters (the clusters don't overlap as much)\n",
    "2. Do you think the the data is spread evenly across the plot, and are the plots potentially misleading and if so why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm just gonna give you the hardcoded B vs T labels and also the platform labels\n",
    "BTcolors = [\"r\"] * 3 + [\"g\"] * 3 + [\"r\"] * 3 + [\"g\"] * 2 + [\"r\"] * 3 + [\"g\"] * 3 + [\"r\"] * 3 + [\"g\"] * 3\n",
    "platform_colors = [\"r\"] * 6 + [\"g\"] * 5 + [\"b\"] * 6 + [\"y\"] * 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some scatter plots of random genes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch effects and scale of features\n",
    "There are a couple of issues with the data as is. Firstly, there are some pretty obvious **batch effects** in the data. Batch effects refer to processes in the data collection stage which result in biases in the data. In genomics terms we talk about **biological signal vs technical signal**. This is specific for genomics and not machine learning in general, but the idea of other sources of variation which can be attributed to other processes that we are not interested in being present is an issue. In this case, the technical signal is a lot stronger than the biological signal due to platform and microarray chip specific variability.\n",
    "\n",
    "Another issue with the data above is the huge variation in scale across the features. If you look at the range of most features, the minimum value is in the single digits while the maximum value may be in the tens of thousands. Often machine learning and statistical algorithms work better on data when the scale of various features are similar. In genomics, the data given is usually passed through a log2 transform.\n",
    "\n",
    "In pandas, there are a bunch of functions **dataframe.apply()** and **dataframe.applymap()** which can be used to apply a function to rows/columns and elements of the dataframe respectively. Using the **np.log2** function, apply a log2 transformation to all the data in the microarray data set. Remember to reassign the output to the variable because these functions don't modify the dataframe directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data using log2\n",
    "\n",
    "\n",
    "# Replot your scatter plots from above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least now the scale of each axis is less misleading. Addressing batch effects is a far more complicated issue which we will not dive into at the moment. However, it is something to keep in mind for the future.\n",
    "\n",
    "## Summary\n",
    "In this handout we have learned another way to explore the data, which is to use scatter plots to observe the relationship between features and what clusters/natural groups in data are. We also introduced the idea of batch effects and scale of features which should be addressed through batch removal and scaling/normalization techniques respectively. The use of logarithms can help to make data easier to handle when they are not linear.\n",
    "\n",
    "## Exercises\n",
    "In the B and T cell data set, there are more than 6000 features. In the previous exercise which involved working on the iris data set, you could test each feature individually and determine which feature was the best for differentiating flower types. In this case it would be implausible to do it manually. One way you could do this would be to do multiple t.tests and compare across all genes. In fact this is what is done in some forms of genomic analysis which try to identify differentially expressed genes. Another way to identify potentially interesting genes is to extract those with a high coefficient of variation. The coefficient of variation is simply the standard deviation of a gene divided by its mean expression. In the exercises below, we will perform these analyses.\n",
    "\n",
    "Let us start by calculating the coefficient of variation for each gene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that accepts a list of expression values from a gene and returns the coefficient of variation\n",
    "\n",
    "# Plot the distribution of the coefficients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the coefficient of variation for the genes in the dataset, we can use this as a threshold to filter for genes with a high coefficient of variation. The reason for using this metric is that interesting genes are normally those which have a wider range in expression (then there is more likely to be a difference in distributions between the classes you are trying to compare). Normalising by the mean is useful because when a gene's mean expression is higher, there it is more likely to have variation in gene expression.\n",
    "\n",
    "Now we would like to perform pairwise t tests on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "# Split the data into 2 data frames, one for B and one for T\n",
    "\n",
    "# Perform t tests for each gene on the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have calculated p values for all these genes, we could use a threshold to identify differentially expressed genes. However, because we performed so many tests, we run into the issue of multiple testing. There are many ways to account for this including calculating a q-value (false discovery rate adjusted p-value) or using threshold adjustment like bonferroni correction. The bonferroni correction is fairly strict, and simply divides your p-value threshold by the number of tests that you have done. We can also perform a scatter plot of **fold-change** against p-value of **significance** to identify the most interesting genes. This is called a volcano plot. **Fold-change** is simply the ratio of the mean gene expression between 2 groups (aka B and T). Also remember to plot the negative log p-value and log the fold change to make the plot more understandable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the fold change for each gene and add it to the data frame containing your p-values\n",
    "\n",
    "# Plot p value against fold change\n",
    "\n",
    "# Plot -log(p-value) against fold change\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data points closer to the top and sides of the second plot are usually the interesting genes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
